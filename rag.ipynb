{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss.cpu\n",
      "  Using cached faiss_cpu-1.8.0.post1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /home/ha1st/venv/Train/lib/python3.12/site-packages (from faiss.cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/ha1st/venv/Train/lib/python3.12/site-packages (from faiss.cpu) (24.1)\n",
      "Using cached faiss_cpu-1.8.0.post1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
      "Installing collected packages: faiss.cpu\n",
      "Successfully installed faiss.cpu-1.8.0.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss.cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering is the process of designing and refining natural language prompts to elicit specific responses or behaviors from language models, such as chatbots, virtual assistants, or other AI systems. It involves tweaking variables like wording, syntax, tone, and context to optimize the model's performance, accuracy, and understanding of the intended task or question. Effective prompt engineering can significantly improve the quality of output, reduce errors, and enhance overall conversational flow between humans and machines.\n",
      "\n",
      " => tokens/second : -1719926916.990373\n"
     ]
    }
   ],
   "source": [
    "def execute(chain): \n",
    "    tokens = \"\"\n",
    "    start = time.time()\n",
    "    for chunk in chain.stream(\"Explain prompt engineering in one small paragraph\"): \n",
    "        print(chunk, end='', flush=True)\n",
    "        tokens += chunk\n",
    "    end = time.time()\n",
    "    print(f\"\\n\\n => tokens/second : {len(tokens.split(\" \"))/(end-start)}\")\n",
    "\n",
    "model = Ollama(model=\"llama3\")\n",
    "execute(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
